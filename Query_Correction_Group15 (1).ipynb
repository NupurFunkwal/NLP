{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Query Correction - Group15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF23-bEkNcio"
      },
      "source": [
        "#Query Formation : Group 15\n",
        "\n",
        "1.   Digvijaysinh Sujaysinh Mane -- 2018A7PS0093G\n",
        "2.   Nupur Funkwal               -- 2018A7PS0624G\n",
        "3.   Mitali Doshi                -- 2019A7PS0064G\n",
        "4.   Nirzari Kalpesh Shah        -- 2019A8PS0576G\n",
        "5.   Prayas Kumar Singh          -- 2019A8PS0439G\n",
        "\n",
        "\n",
        "\n",
        "#Files submitted along with the code:\n",
        "Output files:\n",
        "\n",
        "*   Output file for *base model* on train data  -- train_result_tri.txt\n",
        "*   Output file for *base model* on test data   -- test_result_tri.txt\n",
        "*   Output file for *final model* on train data -- train_file.txt\n",
        "*   Output file for *final model* on test data. -- test_result.txt\n",
        "\n",
        "\n",
        "Final presentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoxB6hHfHz9i"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBILL8JhHwdJ",
        "outputId": "f012a399-a66d-4386-eab7-11d6643084a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"/content/drive/MyDrive/NLP/qac_background.txt\" \"corpus.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JVQBHsLIGSY",
        "outputId": "c46881a4-d296-42d3-e6d2-e0a6e067352f"
      },
      "source": [
        "#required libraries \n",
        "!pip install rank_bm25 nltk\n",
        "import spacy\n",
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "import pandas as pd\n",
        "nlp = spacy.load('en')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "from nltk import word_tokenize\n",
        "from nltk import everygrams\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# loading data into respective files\n",
        "training = pd.read_csv('/content/drive/MyDrive/NLP/qac_training.tsv', sep='\\t', header = None)\n",
        "test = pd.read_csv('/content/drive/MyDrive/NLP/qac_test.tsv', sep='\\t', header = None)\n",
        "validation = pd.read_csv('/content/drive/MyDrive/NLP/qac_validation.tsv', sep='\\t', header = None)\n",
        "\n",
        "x = training.loc[:, 0]    #Column A\n",
        "y = training.loc[:, 1]    #Column B\n",
        "\n",
        "data = training.loc[:, 1].unique()\n",
        "for i in range(len(data)):\n",
        "  data[i]+=(\" $\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vz8n6UMIZmF"
      },
      "source": [
        "# Part 1: Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L8pfVksIewK"
      },
      "source": [
        "Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2CgZC7bIYoX"
      },
      "source": [
        "def tokenize_queries():\n",
        "  corpus = open(\"corpus.txt\", 'r')\n",
        "  \n",
        "  tokenized_queries = list()\n",
        "  for query in corpus.readlines() :\n",
        "    tokens = ['<s>']\n",
        "\n",
        "    for token in query.split():\n",
        "      tokens.append(token)\n",
        "\n",
        "    tokens.append('<e>')\n",
        "    \n",
        "    tokenized_queries.append(tokens)\n",
        "  \n",
        "  return tokenized_queries\n",
        "\n",
        "\n",
        "def get_token_freqency(tokenized_queries) :\n",
        "  token_freqency = dict()\n",
        "\n",
        "  for query in tokenized_queries :\n",
        "    for token in query:\n",
        "      if token in token_freqency:\n",
        "        token_freqency[token] += 1\n",
        "      else:\n",
        "        token_freqency.update({ token : 1 })\n",
        "\n",
        "  return token_freqency\n",
        "\n",
        "\n",
        "def get_vocab(token_freqency, count_threshold = 10.0, count_limit = float('inf')):\n",
        "\n",
        "  vocab = set()\n",
        "\n",
        "  for token, count in token_freqency.items():\n",
        "    if count > count_threshold and count < count_limit:\n",
        "      vocab.add(token);\n",
        "\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def build_bigram_model(tokenized_queries):\n",
        "  bigram_model = dict()\n",
        "\n",
        "  for query in tokenized_queries :\n",
        "    for i in range(len(query) - 1):\n",
        "      bigram = query[i] + query[i+1]\n",
        "      if bigram in bigram_model:\n",
        "        bigram_model[bigram] += 1\n",
        "      else:\n",
        "        bigram_model.update({ bigram : 1 })\n",
        "\n",
        "  for bigram in bigram_model.keys():\n",
        "    bigram_model[bigram] = math.log(bigram_model[bigram])\n",
        "\n",
        "  return bigram_model\n",
        "\n",
        "\n",
        "## edit distance (levenshtein distance)\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "  \n",
        "  # 2 - D matrix 'd'\n",
        "  # x = [[val for i in range(N)] for j in range(M)\n",
        "  m=len(s1)\n",
        "  n=len(s2)\n",
        "  d=[[0 for i in range(0,m+1)]for j in range(0,n+1)]\n",
        "\n",
        "  for i in range(0, m+1):\n",
        "    d[0][i]=i\n",
        "  for j in range(0, n+1):\n",
        "    d[j][0]=j\n",
        "\n",
        "  for i in range(1, m+1):\n",
        "    for j in range(1, n+1):\n",
        "      if s1[i-1]==s2[j-1]:\n",
        "        d[j][i]=d[j-1][i-1]\n",
        "      else:\n",
        "        d[j][i]=min(d[j-1][i]+1, d[j][i-1]+1, d[j-1][i-1]+1)\n",
        "\n",
        "  return d[n][m]\n",
        "\n",
        "\n",
        "def get_query_prob(query, bigram_model):\n",
        "  prob = 0\n",
        "  for i in range(len(query) - 1):\n",
        "    bigram = query[i] + query[i+1]\n",
        "    if bigram in bigram_model:\n",
        "      prob += bigram_model[bigram]\n",
        "\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0--u-Q9vJCC4"
      },
      "source": [
        "tokenized_queries = tokenize_queries()\n",
        "token_freqency = get_token_freqency(tokenized_queries)\n",
        "vocab = get_vocab(token_freqency)\n",
        "bigram_model = build_bigram_model(tokenized_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGYU8f9Jrj1"
      },
      "source": [
        "Correction Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwiM-l0qJT0f"
      },
      "source": [
        "def do_spell_check(incorrect_word, dictionary, max_edit_distance):\n",
        "  output_list=[]\n",
        "  for word in dictionary:\n",
        "    if abs(len(word)-len(incorrect_word))<=max_edit_distance:\n",
        "      if levenshtein_distance(word, incorrect_word)<=max_edit_distance:\n",
        "        output_list.append(word)\n",
        "  \n",
        "  return output_list\n",
        "\n",
        "\n",
        "def get_simmilar_words(word, vocab, edit_distance = 2.0, cutoff_factor = 0.9):\n",
        "  if cutoff_factor > 1 or cutoff_factor < 0:\n",
        "      raise Exception(\"cutoff frequency needs to be [0,1]\") \n",
        "\n",
        "  guesses = do_spell_check(word, vocab, edit_distance)\n",
        "  max_freq = 0\n",
        "  for word in guesses:\n",
        "    if max_freq < token_freqency[word]:\n",
        "      max_freq = token_freqency[word]\n",
        "\n",
        "  candidate_words = list()\n",
        "  for word in guesses:\n",
        "    if token_freqency[word] >= cutoff_factor * max_freq:\n",
        "      candidate_words.append(word)\n",
        "\n",
        "  if '<s>' in candidate_words:\n",
        "    candidate_words.remove('<s>')\n",
        "  if '<e>' in candidate_words:\n",
        "    candidate_words.remove('<e>')\n",
        "\n",
        "  return candidate_words\n",
        "\n",
        "\n",
        "def context_based_spell_correction(error_word, word_before, word_after, vocab, bigram_model, edit_distance = 2.0, cutoff_factor = 0.0): \n",
        "  candidate_words = get_simmilar_words(error_word, vocab, edit_distance, cutoff_factor)\n",
        "\n",
        "  max_prob = -1\n",
        "  suggested_word = \"\"\n",
        "  for word in candidate_words:\n",
        "\n",
        "    prob = 0\n",
        "    bigram = word_before + word\n",
        "    if bigram in bigram_model:\n",
        "      prob1 = bigram_model[bigram]\n",
        "    else:\n",
        "      prob1 = 0\n",
        "\n",
        "    bigram = word + word_after\n",
        "    if bigram in bigram_model:\n",
        "      prob2 = bigram_model[bigram]\n",
        "    else:\n",
        "      prob2 = 0\n",
        "\n",
        "    if max_prob < prob1 + prob2:\n",
        "      max_prob = prob1 + prob2\n",
        "      suggested_word = word\n",
        "\n",
        "  return suggested_word\n",
        "\n",
        "\n",
        "def query_correction(query_str, vocab, bigram_model, edit_distance = 2.0, cutoff_factor = 0.8):\n",
        "  query = query_str.split()\n",
        "\n",
        "  query.insert(0, '<s>')\n",
        "  query.append('<e>')\n",
        "\n",
        "  #spell check of each word in query\n",
        "  for i in range(1, len(query) - 1):\n",
        "    if query[i] not in vocab:\n",
        "      w = context_based_spell_correction(query[i], query[i-1], query[i+1], vocab, bigram_model, 1, cutoff_factor)\n",
        "      if not w:\n",
        "        query[i] = '<unk>'\n",
        "      else:\n",
        "        query[i] = w\n",
        "\n",
        "  #simmilar spelling word check using bigram\n",
        "  candidate_words = list()\n",
        "  for word in query[1:-1]:\n",
        "    candidate_words.append( get_simmilar_words(word, vocab, edit_distance, cutoff_factor ))\n",
        "  \n",
        "  corrected_query = query\n",
        "  max_query_prob = get_query_prob(query, bigram_model)\n",
        "  for i in range(1, len(query) - 1):\n",
        "    for word in candidate_words[i - 1]:\n",
        "      candidate_query = query[0:i] + [word] + query[i+1:]\n",
        "      query_prob = get_query_prob(candidate_query, bigram_model)\n",
        "\n",
        "      if max_query_prob < query_prob:\n",
        "        max_query_prob = query_prob\n",
        "        corrected_query = candidate_query\n",
        "\n",
        "  return corrected_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfjzYrTJx4E"
      },
      "source": [
        "\n",
        "### spelling correct\n",
        "* air conditioner *fillers*\n",
        "* gta san andreas *ceat* codes\n",
        "\n",
        "### all words of query incorrect\n",
        "* psycological disordr\n",
        "* oline spel chcker\n",
        "\n",
        "### context based word correction\n",
        "* *whether* in london\n",
        "* make *monkey* at home\n",
        "\n",
        "### spelling and context based corrections\n",
        "* *back* of america visa *crrd*\n",
        "* what is the *tame* in *jpan*\n",
        "\n",
        "### limitations\n",
        "* marry jane watson\n",
        "* basketball hall of fame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h021DXJJvJe",
        "outputId": "b6e68c7b-22e6-4b9d-a219-509b44bc0297"
      },
      "source": [
        "queries = ['whether in london', 'air conditioner fillers', 'oline spel chcker', 'back of america visa crrd']\n",
        "for q in queries: \n",
        "  print(query_correction(q, vocab, bigram_model, 2, 0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'weather', 'in', 'london', '<e>']\n",
            "['<s>', 'air', 'conditioner', 'filters', '<e>']\n",
            "['<s>', 'online', 'spell', 'checker', '<e>']\n",
            "['<s>', 'bank', 'of', 'america', 'visa', 'card', '<e>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiYKJYs6LIyc"
      },
      "source": [
        "#Part 2: Query Completion\n",
        "Trained model using the training data to complete incomplete queries for the qac_training.tsv file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEJxH_PfXYxC"
      },
      "source": [
        "# Preprocessing: On both test and train datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNPRQN_iXdqG",
        "outputId": "71e814bc-c22b-47b3-fe89-d28c956df5d2"
      },
      "source": [
        "#data == training data\n",
        "data = training.loc[:, 1].unique()\n",
        "for i in range(len(data)):\n",
        "  data[i]+=(\" $\")\n",
        "\n",
        "#test_data == test data\n",
        "test_data = test.loc[:,1].unique()\n",
        "for i in range(len(test_data)):\n",
        "  test_data[i]+=(\" $\")\n",
        "\n",
        "\n",
        "#the following preprocessing steps were carried out to find the intersection of data between train and testing data\n",
        "#the following is an instance of utilising Porter's Stemmer, we ended up exploring Snowball Stemmer as well as Lemmatization\n",
        "#the results were very similar with intersection being close to 1750 words for all three methods\n",
        "\n",
        "train_stem = []\n",
        "test_stem = []\n",
        "\n",
        "for query in data:\n",
        "  words = word_tokenize(query)\n",
        "  for w in words:\n",
        "      if w not in train_stem:\n",
        "        train_stem.append(stemmer.stem(w))\n",
        "\n",
        "for query in test_data:\n",
        "  words = word_tokenize(query)\n",
        "  for w in words:\n",
        "      if w not in test_stem:\n",
        "        test_stem.append(stemmer.stem(w))\n",
        "\n",
        "#returns intersection of two lists\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "print(\"Common tokens in test and training=\",len(intersection(train_stem, test_stem)))\n",
        "print(\"Total tokens in test=\", len(test_stem))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common tokens in test and training= 1725\n",
            "Total tokens in test= 4461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJXaG-pPX-GS",
        "outputId": "904cebdc-649f-443b-878e-fe0403142a51"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "train_lem = []\n",
        "test_lem = []\n",
        "\n",
        "test_data = test.loc[:,1].unique()\n",
        "for i in range(len(test_data)):\n",
        "  test_data[i]+=(\" $\")\n",
        "\n",
        "for query in data:\n",
        "  words = word_tokenize(query)\n",
        "  for w in words:\n",
        "      if w not in train_lem:\n",
        "        train_lem.append(lemmatizer.lemmatize(w))\n",
        "\n",
        "for query in test_data:\n",
        "  words = word_tokenize(query)\n",
        "  for w in words:\n",
        "      if w not in test_lem:\n",
        "        test_lem.append(lemmatizer.lemmatize(w))\n",
        "\n",
        "#returns intersection of two lists\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "print('Common tokens in test and training=',len(intersection(train_lem, test_lem)))\n",
        "print('Total tokens in test=',len(test_lem))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Common tokens in test and training= 1698\n",
            "Total tokens in test= 4090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ql55p65YUTU"
      },
      "source": [
        "#Character Gram Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZSiyFm8YYm4"
      },
      "source": [
        "#baseline model: utilises trigram\n",
        "def char_prediction_trigram(query, k_gram_model,flag):\n",
        "  #query is to be treated as a string, so we need not make a list of tokens separately\n",
        "  length=len(query)\n",
        "\n",
        "  #base case: 2; the string is an empty string\n",
        "  if (not(length)):\n",
        "    return [(1,\"\")]\n",
        "\n",
        "  #since the function is recursive, this is the base case: query is complete: return probability of the query and the query itself\n",
        "  if (flag and query[length-1]==' '):\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "  if (query[length-1]=='$'):\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "\n",
        "  key=[]\n",
        "  for i in range(length):\n",
        "    key.append(query[i])\n",
        "\n",
        "  candidate_tokens=[]\n",
        "  k=3\n",
        "\n",
        "  while(len(key)>k-1):\n",
        "    key.pop(0)\n",
        "\n",
        "  #recheck\n",
        "  while (k > 2):\n",
        "    if tuple(key) not in k_gram_model[k].keys():\n",
        "      # print(key, end=\" \")\n",
        "      # print(k)\n",
        "      key.pop(0)\n",
        "      k-=1\n",
        "    else:\n",
        "      candidate_dict = k_gram_model[k][tuple(key)]\n",
        "      break\n",
        "\n",
        "  if k==2:\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "\n",
        "  resultant_queries = []\n",
        "  for i in list(candidate_dict.keys()):\n",
        "    new_query = query + i\n",
        "    resultant_queries += char_prediction(new_query, k_gram_model,flag)\n",
        "    \n",
        "  return Sort_Tuple(resultant_queries)[0:10]\n",
        "\n",
        "\n",
        "#advanced Character Gram model with back-off and higher k-gram models\n",
        "#input data\n",
        "char_data = training.loc[:, 1].unique()\n",
        "for i in range(len(char_data)):\n",
        "  char_data[i]+=(\" $\")\n",
        "\n",
        "#builds n_gram character models\n",
        "def n_gram_model_character(n, char_data):\n",
        "  n_gram_model=defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  for sentence in char_data:\n",
        "    l = len(sentence)\n",
        "    if (l <= n-2):\n",
        "      continue\n",
        "    n_grams=list(everygrams(sentence,n,n))\n",
        "    \n",
        "    # making key\n",
        "    for words in n_grams:\n",
        "      key_list=[]\n",
        "      for x in range(len(words)-1):\n",
        "        key_list.append(words[x])\n",
        "      key_list=tuple(key_list)\n",
        "      n_gram_model[key_list][words[n-1]]+=1\n",
        "\n",
        "  for x in n_gram_model:\n",
        "    total_count = float(sum(n_gram_model[x].values()))\n",
        "    for w in n_gram_model[x]:\n",
        "      n_gram_model[x][w] /= total_count\n",
        "\n",
        "  return n_gram_model\n",
        "\n",
        "\n",
        "k_gram_model_char=[0,0,0]\n",
        "\n",
        "for i in range(3,31):\n",
        "  k_gram_model_char.append(n_gram_model_character(i,char_data))\n",
        "\n",
        "\n",
        "#helper functions \n",
        "import math\n",
        "\n",
        "#sorts list of tuples\n",
        "def Sort_Tuple(tup): \n",
        "    tup.sort(key = lambda x: -x[0]) \n",
        "    return tup \n",
        "\n",
        "#calculates frequency for each char in data\n",
        "def calculate_freq_char(data):\n",
        "  tokenized_queries = []\n",
        "  for query in data :\n",
        "    tokens = []\n",
        "    for token in query:\n",
        "      tokens.append(token)\n",
        "    tokenized_queries.append(tokens)\n",
        "\n",
        "  tokens_count = {}\n",
        "\n",
        "  for query in tokenized_queries :\n",
        "    for token in query:\n",
        "      if token in tokens_count:\n",
        "        tokens_count[token] += 1\n",
        "      else:\n",
        "        tokens_count.update({ token : 1 })\n",
        "\n",
        "  return tokens_count\n",
        "\n",
        "\n",
        "char_count = calculate_freq_char(char_data)\n",
        "\n",
        "\n",
        "#calculates probability for the entire query\n",
        "def calculate_char_probability(query_words):\n",
        "  l=len(query_words)\n",
        "  total_count = 0\n",
        "  for token in char_count.keys():\n",
        "    total_count += char_count[token]\n",
        "\n",
        "  prob = 10**10\n",
        "  prob *= (char_count[query_words[0]]/total_count)\n",
        "\n",
        "  for i in range(2,l):\n",
        "    prob *= (k_gram_model_char[3][(query_words[i-2],query_words[i-1])][query_words[i]])\n",
        "\n",
        "  return prob\n",
        "\n",
        "\n",
        "#char prediction\n",
        "def char_prediction(query, k_gram_model, flag):\n",
        "  #query is to be treated as a string, so we need not make a list of tokens separately\n",
        "  length=len(query)\n",
        "\n",
        "  #base case: 2; the string is an empty string\n",
        "  if (not(length)):\n",
        "    return [(1,\"\")]\n",
        "\n",
        "  #since the function is recursive, this is the base case: query is complete: return probability of the query and the query itself\n",
        "  if (flag and query[length-1]==' '):\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "  if (query[length-1]=='$'):\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "\n",
        "  \n",
        "  key=[]\n",
        "  for i in range(length):\n",
        "    key.append(query[i])\n",
        "\n",
        "  candidate_tokens=[]\n",
        "  k=30\n",
        "  if length < 29:\n",
        "    k=length+1\n",
        "\n",
        "  while(len(key)>k-1):\n",
        "    key.pop(0)\n",
        "\n",
        "  #recheck\n",
        "  while (k > 2):\n",
        "    if tuple(key) not in k_gram_model[k].keys():\n",
        "      # print(key, end=\" \")\n",
        "      # print(k)\n",
        "      key.pop(0)\n",
        "      k-=1\n",
        "    else:\n",
        "      candidate_dict = k_gram_model[k][tuple(key)]\n",
        "      break\n",
        "\n",
        "  if k==2:\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "\n",
        "  resultant_queries = []\n",
        "  for i in list(candidate_dict.keys()):\n",
        "    new_query = query + i\n",
        "    resultant_queries += char_prediction(new_query, k_gram_model,flag)\n",
        "    \n",
        "  return Sort_Tuple(resultant_queries)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp59WWwKLnW4"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8LFKr-LKxwK"
      },
      "source": [
        "#making a dictionary to complete word\n",
        "def mapping_from_right(corpus):\n",
        "  #traversing through the corpus to assist with edit distance\n",
        "  final_dict={}\n",
        "  for word in corpus:\n",
        "    for l in range(len(word)):\n",
        "      #start will be always 0\n",
        "      #end will be l+1\n",
        "      key = word[0:l+1]\n",
        "      if key in final_dict.keys():\n",
        "        final_dict[key].append(word)\n",
        "      else:\n",
        "        final_dict[key]=[]\n",
        "        final_dict[key].append(word)\n",
        "  return final_dict\n",
        "\n",
        "\n",
        "#helper function to make a bigram model, parameter taken is a list of sentences\n",
        "#returns a dictionary of dictionaries\n",
        "#first key is the first word in bigram; the second key the second word in bigram and the value is the frequency of the subsequently formed bigram\n",
        "def make_bigram(data):\n",
        "  bigram_model={}\n",
        "\n",
        "  for sentence in data:\n",
        "    l = len(sentence.split())\n",
        "    if (l <= 0):\n",
        "      continue\n",
        "    bi_grams=list(everygrams(sentence.split(),2,2))\n",
        "\n",
        "    for t in bi_grams:\n",
        "      if (t[0] in bigram_model.keys() and t[1] in bigram_model[t[0]].keys()):\n",
        "        bigram_model[t[0]][t[1]]+=1\n",
        "      elif (t[0] not in bigram_model.keys()):\n",
        "        bigram_model[t[0]]={}\n",
        "        bigram_model[t[0]][t[1]]=1\n",
        "      elif (t[1] not in bigram_model[t[0]].keys()):\n",
        "        bigram_model[t[0]][t[1]]=1\n",
        "      \n",
        "\n",
        "  for x in bigram_model:\n",
        "    total_count = float(sum(bigram_model[x].values()))\n",
        "    for w in bigram_model[x]:\n",
        "      bigram_model[x][w] /= total_count\n",
        "        \n",
        "  return bigram_model\n",
        "\n",
        "\n",
        "def n_gram_model(n, data):\n",
        "  n_gram_model=defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  for sentence in data:\n",
        "    l = len(sentence.split())\n",
        "    if (l <= n-2):\n",
        "      continue\n",
        "    n_grams=list(everygrams(sentence.split(),n,n))\n",
        "    \n",
        "    # making key\n",
        "    for words in n_grams:\n",
        "      key_list=[]\n",
        "      for x in range(len(words)-1):\n",
        "        key_list.append(words[x])\n",
        "      key_list=tuple(key_list)\n",
        "      n_gram_model[key_list][words[n-1]]+=1\n",
        "\n",
        "  for x in n_gram_model:\n",
        "    total_count = float(sum(n_gram_model[x].values()))\n",
        "    for w in n_gram_model[x]:\n",
        "      n_gram_model[x][w] /= total_count\n",
        "\n",
        "  return n_gram_model\n",
        "\n",
        "\n",
        "#list of k_gram_models\n",
        "#bigram model will be k_gram_model[2], trigram model will be k_gram_model[3] and so on\n",
        "k_gram_model=[0,0]\n",
        "#the function for making a bigram model is entirely different\n",
        "k_gram_model.append(make_bigram(data))\n",
        "for i in range(3,10):\n",
        "  k_gram_model.append(n_gram_model(i,data))\n",
        "\n",
        "\n",
        "#returns intersection of two lists\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "\n",
        "#calculates freuency for each token in data\n",
        "def calculate_freq(data):\n",
        "  tokenized_queries = []\n",
        "  for query in data :\n",
        "    tokens = []\n",
        "    for token in query.split():\n",
        "      tokens.append(token)\n",
        "    tokenized_queries.append(tokens)\n",
        "\n",
        "  tokens_count = {}\n",
        "\n",
        "  for query in tokenized_queries :\n",
        "    for token in query:\n",
        "      if token in tokens_count:\n",
        "        tokens_count[token] += 1\n",
        "      else:\n",
        "        tokens_count.update({ token : 1 })\n",
        "\n",
        "  return tokens_count\n",
        "\n",
        "\n",
        "#calculates probability for the entire query\n",
        "def calculate_query_probability(query):\n",
        "  query_words=query.split()\n",
        "  l=len(query_words)\n",
        "  total_count = 0\n",
        "  for token in tokens_count.keys():\n",
        "    total_count += tokens_count[token]\n",
        "\n",
        "  prob = 10**10\n",
        "  prob *= tokens_count[query_words[0]]/total_count\n",
        "\n",
        "  for i in range(1,l):\n",
        "    prob *= k_gram_model[2][query_words[i-1]][query_words[i]]\n",
        "\n",
        "  return prob\n",
        "\n",
        "\n",
        "tokens_count = calculate_freq(data)\n",
        "\n",
        "\n",
        "#returns list of tuple of probabilities and the final queries\n",
        "#here the flag means that the last word of input is complete or not\n",
        "def query_prediction(query, k_gram_model, threshold, flag):\n",
        "  tokens = []\n",
        "  for token in query.split():\n",
        "    tokens.append(token)\n",
        "  \n",
        "  length = len(tokens)\n",
        "\n",
        "  if (query[length-1] == \"$\"):\n",
        "    return [(calculate_char_probability(query), query)]\n",
        "\n",
        "  #for incomplete last word in query\n",
        "  if flag == False:\n",
        "    resultant_queries = []\n",
        "    #list of possible correct last words        --> some threshold using edit distance can be set here\n",
        "    x = mapping_from_right(list(tokens_count.keys()))\n",
        "    if tokens[length-1] in x.keys():\n",
        "      possible_words = x[tokens[length-1]]\n",
        "\n",
        "    #check if these possible words have atleast a bigram relation\n",
        "    if length> 1 and tokens[length-2] in k_gram_model[2].keys():\n",
        "      bi_words = list(k_gram_model[2][tokens[length-2]].keys())\n",
        "      possible_words = intersection(possible_words, bi_words)\n",
        "    #print('pos=', possible_words)\n",
        "\n",
        "    for i in possible_words:\n",
        "      tokens[length-1] = i\n",
        "      new_query = ''\n",
        "      for t in tokens:\n",
        "        new_query += (t+' ')\n",
        "\n",
        "      new_query = new_query[:-1]\n",
        "      #print('new query=', new_query)\n",
        "      resultant_queries += query_prediction(new_query, k_gram_model, threshold, True)\n",
        "    \n",
        "    return Sort_Tuple(resultant_queries)[0:10]\n",
        "\n",
        "  else:\n",
        "    #getting candidate tokens\n",
        "    if length > 1:\n",
        "      key = []\n",
        "      for i in range(length):\n",
        "        key.append(tokens[i])\n",
        "      #key = tuple(key)\n",
        "    \n",
        "    else:\n",
        "      key = (tokens[0])\n",
        " \n",
        "    #base case\n",
        "    # if length>1:\n",
        "    #   if tuple(key) not in list(k_gram_model[length+1].keys()):\n",
        "    #     return [(calculate_query_probability(query), query)]\n",
        "\n",
        "    if length == 1:\n",
        "      if key not in list(k_gram_model[2].keys()):\n",
        "        return [(calculate_char_probability(query), query)]\n",
        "\n",
        "    candidate_tokens = []\n",
        "    if length < 8:                   \n",
        "      k = length+1\n",
        "    else: \n",
        "      k = 9\n",
        "\n",
        "    while(len(key)>k-1):\n",
        "      if isinstance(key,list):\n",
        "        key.pop(0)\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # going for smaller k grams if key not found in bigger one \n",
        "    while(k>1):\n",
        "        if k>2 and tuple(key) not in list(k_gram_model[k].keys()):\n",
        "          key.pop(0)\n",
        "          k -= 1\n",
        "        elif k==2:\n",
        "          if key in list(k_gram_model[k].keys()):\n",
        "            candidate_dict = k_gram_model[k][key]\n",
        "            break\n",
        "          else:\n",
        "            k -= 1\n",
        "        else:\n",
        "          candidate_dict = k_gram_model[k][tuple(key)]\n",
        "          break\n",
        "\n",
        "    if k==1:\n",
        "      return [(calculate_char_probability(query), query)]\n",
        "    \n",
        "    #base case: empty candidate_dict \n",
        "    if bool(candidate_dict) == False:\n",
        "      return [(calculate_char_probability(query), query)]\n",
        "\n",
        "    resultant_queries = []\n",
        "    for i in candidate_dict.keys():\n",
        "      if candidate_dict[i] > threshold:\n",
        "        new_query = query + ' ' + i\n",
        "        resultant_queries += query_prediction(new_query, k_gram_model, threshold, True)\n",
        "    \n",
        "    return Sort_Tuple(resultant_queries)[0:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ_cxahVMEAP",
        "outputId": "3477416c-08d2-4956-f55a-3d3cdd008698"
      },
      "source": [
        "#The function query_prediction() uses only the word-gram model\n",
        "#This function is further called in final_pred() coded below\n",
        "\n",
        "print(query_prediction('www', k_gram_model, 0, False))\n",
        "print(query_prediction('hampt', k_gram_model, 0, False))\n",
        "print(query_prediction('weddings', k_gram_model, 0.25, True))\n",
        "print(query_prediction('uniq', k_gram_model, 0.25, False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(69305.96973388329, 'www my $'), (33717.71513841273, 'www mys $'), (17314.080336879633, 'www mas $'), (13157.394187558324, 'www e $'), (6502.942835987436, 'www mr $'), (4538.681630739294, 'www g4 $'), (4470.236697568606, 'www mtv $'), (2157.001730532563, 'www eb $'), (1673.1966801772867, 'www jobs com $'), (1368.4686770285891, 'www das $')]\n",
            "[(0.006182789966610348, 'hampton house $'), (4.277629755050705e-08, 'hamptonmaidinn com $'), (1.3422695697602991e-10, 'hampton hotel york pa $'), (1.3764293929755488e-23, 'hampton beach nh chamber of commerce $')]\n",
            "[(6.6080187974016625e-09, 'weddings las vegas $'), (9.98337643638317e-19, 'weddings in central park new york $')]\n",
            "[(0.011935343850234863, 'unique marine $'), (0.010927045196449292, 'unique thongs $'), (1.2090403337904697e-11, 'unique vinyl siding $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZn7f4_9ZSDX"
      },
      "source": [
        "#calculation of Perplexity to quantify the model\n",
        "def find_perplexity(resultant_query):\n",
        "  # print(resultant_query)\n",
        "  # print(type(resultant_query[1]))\n",
        "  prob=calculate_char_probability(resultant_query[1])\n",
        "  return prob**((-1)/len(resultant_query))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6dk16-ZnAt"
      },
      "source": [
        "#Final Prediction of The Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n_zt0DaZSMH"
      },
      "source": [
        "#first complete the last word using char gram model till we get space or $\n",
        "#then we check for word gram model\n",
        "#if that doesnt work, then use char gram model\n",
        "#in case of exception, try using edit distance\n",
        "\n",
        "def check_for_word_gram(query,k_gram_model):\n",
        "  query_words=query.split()\n",
        "  # print(query_words)\n",
        "  \n",
        "  while len(query_words):\n",
        "    # print(len(query_words))\n",
        "    if (len(query_words)==1):\n",
        "      if(query_words[0] in k_gram_model[2].keys()):\n",
        "        # print('word=', query_words[0])\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    elif len(query_words)+1 < 10 and tuple(query_words) in k_gram_model[len(query_words)+1].keys():\n",
        "      return True\n",
        "    else:\n",
        "      query_words.pop(0)\n",
        "    \n",
        "  return False  \n",
        "\n",
        "\n",
        "#final prediction of a query\n",
        "def final_pred(query):\n",
        "  temp_query = \"\"\n",
        "  for i in query:\n",
        "    if i.isalpha() or i==\" \":\n",
        "      temp_query += i\n",
        "\n",
        "  query = temp_query.strip(\" \")\n",
        "  if(len(query) == 0):\n",
        "    return []\n",
        "\n",
        "  final_ans = []\n",
        "  candidate_queries=[]\n",
        "\n",
        "  flag=True\n",
        "  tokens = query.split()\n",
        "  length = len(tokens)\n",
        "  # print('tokens=', tokens)\n",
        "  x = mapping_from_right(list(tokens_count.keys()))\n",
        "  possible_words = []\n",
        "  if tokens[length-1] in x.keys():\n",
        "    possible_words = mapping_from_right(list(tokens_count.keys()))[tokens[length-1]]\n",
        "\n",
        "  # print('possible words1=',possible_words)\n",
        "  if length> 1:\n",
        "    if tokens[length-2] in k_gram_model[2].keys():\n",
        "      bi_words = list(k_gram_model[2][tokens[length-2]].keys())\n",
        "      possible_words = intersection(possible_words, bi_words)\n",
        "    # print('pos=', possible_words)\n",
        "    else:\n",
        "      possible_words = []\n",
        "\n",
        "  # print('possible words2=',possible_words)\n",
        "  resultant_queries = []\n",
        "\n",
        "  if len(possible_words):\n",
        "    flag = False\n",
        "    \n",
        "  for i in possible_words:\n",
        "    tokens[length-1] = i\n",
        "    new_query = ''\n",
        "    for t in tokens:\n",
        "      new_query += (t+' ')\n",
        "    new_query = new_query[:-1]\n",
        "    resultant_queries += query_prediction(new_query, k_gram_model, 0, True)\n",
        "    \n",
        "  if flag==False:\n",
        "    # return Sort_Tuple(resultant_queries)[0:10]\n",
        "    final_ans += Sort_Tuple(resultant_queries)[0:10]\n",
        "      \n",
        "  else:    #means now complete the last word with char gram model\n",
        "    # char prediction - flag == true\n",
        "    l1=char_prediction(query,k_gram_model_char, True)\n",
        "    for tup in l1:\n",
        "      if(tup[1][len(tup[1])-1]=='$'):\n",
        "        final_ans.append(tup)\n",
        "      else:\n",
        "        candidate_queries.append(tup[1])\n",
        "  \n",
        "  for q in candidate_queries:\n",
        "    if (check_for_word_gram(q, k_gram_model)):\n",
        "      ans=query_prediction(q,k_gram_model,0,False)\n",
        "      final_ans+=ans\n",
        "    else:\n",
        "      ans=char_prediction(q,k_gram_model_char,False)\n",
        "      final_ans+=ans\n",
        "  \n",
        "  return Sort_Tuple(final_ans)[0:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vtad4ggjdee"
      },
      "source": [
        "#Example Runs of Advanced model and baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3FNnjO3iHl5",
        "outputId": "b2b7642a-c8b9-4292-bdde-d7334b0ac29c"
      },
      "source": [
        "#advanced model\n",
        "final_pred(\"hamp\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.006182789966610348, 'hampton house $'),\n",
              " (4.277629755050705e-08, 'hamptonmaidinn com $'),\n",
              " (1.3422695697602991e-10, 'hampton hotel york pa $'),\n",
              " (1.3764293929755488e-23, 'hampton beach nh chamber of commerce $')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sA8DmJB676-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fc047b-696e-49bc-82ad-768f273513dc"
      },
      "source": [
        "#baseline model used for comparision\n",
        "char_prediction_trigram(\"hamp\",k_gram_model_char,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(47700.60894018203, 'hamp $'),\n",
              " (33403.384630004606, 'hamps $'),\n",
              " (9838.50260473746, 'hamp3 $'),\n",
              " (7348.6023751243265, 'hample $'),\n",
              " (5201.339773675949, 'hampa $'),\n",
              " (1044.6454083666958, 'hampa com $'),\n",
              " (186.33537477714947, 'hampion $'),\n",
              " (157.52613908431783, 'hampoo $'),\n",
              " (6.889626245459942, 'hampaign $'),\n",
              " (3.9371907209985526, 'hamp parts $')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boBi51l1do57",
        "outputId": "70d36587-8fbf-4e64-b6ca-4ace4bca85c5"
      },
      "source": [
        "#advanced model\n",
        "final_pred(\"time\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(435177.5374343186, 'times $'),\n",
              " (379109.89080131997, 'time $'),\n",
              " (0.17840196797098584, 'time 5 4 06 $'),\n",
              " (0.004731833469136794, 'time results $'),\n",
              " (0.0020234611464014095, 'time slogan $'),\n",
              " (0.0008440999123392078, 'timemagazine com $'),\n",
              " (1.9043097285251848e-06, 'times picayune $'),\n",
              " (9.702639727348005e-07, 'time to get over $'),\n",
              " (5.516976911323449e-08, 'time out magazine $'),\n",
              " (1.5283901305078644e-08, 'timewarnercable $')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrcFjnOIdx0H",
        "outputId": "833529d0-f5d5-4b79-c272-f8cd1346bf60"
      },
      "source": [
        "#baseline model used for comparision\n",
        "char_prediction_trigram(\"time\",k_gram_model_char,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(435177.5374343186, 'times $'),\n",
              " (379109.89080131997, 'time $'),\n",
              " (42620.59683090194, 'times com $'),\n",
              " (2142.400624236118, 'timeo $'),\n",
              " (1760.6830224062755, 'timez com $'),\n",
              " (650.6732936318786, 'timeals $'),\n",
              " (472.16449030129365, 'timeters $'),\n",
              " (141.61220540853986, 'times org $'),\n",
              " (121.22413044259272, 'timeet com $'),\n",
              " (38.780807696576, 'timepage $')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VlzWfsqjtXv"
      },
      "source": [
        "#Running final_pred() on test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l56oEMmhbUK0"
      },
      "source": [
        "test_data = test.loc[:, 0]\n",
        "# # print(test_data)\n",
        "j=0\n",
        "i=0\n",
        "\n",
        "while(j<len(test_data)):\n",
        "  try:\n",
        "    for i in range(j, len(test_data)):\n",
        "      print(\"New Query Start: \", end=\" \")\n",
        "      print(test_data[i])\n",
        "      # preprocess the query\n",
        "      # q_final=query_correction(test_data[i], vocab, bigram_model, 2, 0.5)\n",
        "      pred=final_pred(test_data[i])\n",
        "      for predictions in pred:\n",
        "        print(\"Prediction:    \", predictions)\n",
        "        print(find_perplexity(predictions))\n",
        "      print(\"/////////////////\\n\")\n",
        "    \n",
        "  except Exception as e:\n",
        "    j=i\n",
        "    try:\n",
        "      #correcting the query\n",
        "      q_final_list=query_correction(test_data[i],vocab,bigram_model,2,0.5)\n",
        "      q_final=\"\"\n",
        "      for i in range(1,len(q_final_list)-1):\n",
        "        q_final+=q_final_list[i]\n",
        "      pred=char_prediction_trigram(q_final, k_gram_model_char, True)\n",
        "      for predictions in pred:\n",
        "        print(\"Prediction:    \", predictions)\n",
        "        print(find_perplexity(predictions))\n",
        "        print(\"/////////////////////\\n\")\n",
        "    except Exception as e2:\n",
        "      print(\"Erroreneous query=\", test_data[j])\n",
        "      print(\"Query No.=\", j)\n",
        "      print(\"Exception = \", e)\n",
        "\n",
        "    j+=1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}